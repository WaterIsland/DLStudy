[new!]
--------------------------------------------------------------
・weight decayを実装
    ＊バッチ学習時に利用可能
    ＊使う場合は、learn_batch.py 参照
        ＋nn_obj.use_weight_decay(0.0001)は、λ = 0.0001 の意味
        ＋λ = 0.001以上では学習は失敗気味
--------------------------------------------------------------

[old!]
--------------------------------------------------------------
・２クラス分類を実装した Multi Layer Network
・XOR 問題を解かせている
・学習は ./learn.py で実行
	＊入力層、中間層、出力層の３層で、ニューロン素子はそれぞれ2、3、10個で、全結合
		＋neuro_obj = mln.Mln().make_neuralnet([2, 3, 1], ['sigmoid', 'sigmoid'], 0.01, solved = 'classification')
	＊bios は全ての層で一定値、学習対象外、近日実装予定
	＊学習率：0.01
	＊活性化関数は、出力層も中間層も sigmoid
	＊訓練データ：input_data = [[0., 0.], [0., 1.], [ 1., 0.], [ 1., 1.]]
        ＋上記４つの訓練データを順番に全て50,000回提示
		＋出力層の更新式で対数を用いているため、'0'が出力される可能性を極力なくす必要があり、
		　現状はsigmoidのみ実装（tanhは導入しない予定）
    ＊教師データ：teach_data = [ [0.], [ 1.], [ 1.], [0.]]
    ＊ネットワークの状態をダンプファイルで出力
        ＋default-xor.dump：初期化直後のネットワークの状態をダンプしたファイル
        ＋learn-xor.dump：学習後のネットワークの状態をダンプしたファイル

	＊訓練データ：mnistの手書き画像をランダムに10000個提示
		＋画素値を 0.0 - 1.0 で規格化した画像を利用
	＊教師データ：mnistの手書き画像のラベルを提示
		＋ラベルを 1x10 の並列に変換して利用
	＊ネットワークの状態をダンプファイルで出力
		＋default-br.dump：初期化直後のネットワークの状態をダンプしたファイル
		＋learn-br.dump：学習後のネットワークの状態をダンプしたファイル

・テストは ./learn.py で実行
    ＊入力値、出力値、誤差を表示
        ＋初期化直後と学習後の結果を表示
    ＊テストデータは学習時に利用した数値をそのまま利用
--------------------------------------------------------------
